{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/RAG-pipline.png\" width=90%><p>使用 LangChain 完成蓝框中的 pipeline </p></center>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 文档解析 - PDF2Text\n",
    "\n",
    "本次比赛提供的 PDF 文档中主要以 **文字** 和 **表格** 内容为主，这里使用在处理表格方面表现不错的 ``pdfplumber`` 包。\n",
    "\n",
    "还可以尝试其他 PDF 处理工具，这些工具的效果各有千秋，下面给出一些参考：\n",
    "\n",
    "- [LangChain v0.3 Documentation - How to load PDFs](https://python.langchain.com/docs/how_to/document_loader_pdf)\n",
    "\n",
    "- [LangChain v0.2 Documentation (中文版) - 如何加载 PDF](https://python.langchain.com/docs/how_to/document_loader_pdf)\n",
    "\n",
    "- [一文打尽PDF解析的工程实践——Langchain-RAG系列(1)](https://zhuanlan.zhihu.com/p/680175740)\n",
    "\n",
    "此外，PDF 文档中可能存在一些图片信息，图片信息不会出现在解析结果中。不过图片中的内容可能用文字描述过，已经隐含在文本信息内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing AF33.pdf\n",
      "Parsing AW18.pdf\n",
      "Parsing AT17.pdf\n",
      "Parsing AY12.pdf\n",
      "Parsing AW10.pdf\n",
      "Parsing AY05.pdf\n",
      "Parsing AW32.pdf\n",
      "Parsing AT02.pdf\n",
      "Parsing AZ08.pdf\n",
      "Parsing AZ01.pdf\n",
      "Parsing AW13.pdf\n",
      "Parsing AW05.pdf\n",
      "Parsing AW24.pdf\n",
      "Parsing AF37.pdf\n",
      "Parsing AT09.pdf\n",
      "Parsing AW08.pdf\n",
      "Parsing AF16.pdf\n",
      "Parsing AT13.pdf\n",
      "Parsing AY02.pdf\n",
      "Parsing AW23.pdf\n",
      "Parsing AW28.pdf\n",
      "Parsing AW03.pdf\n",
      "Parsing AW07.pdf\n",
      "Parsing AT21.pdf\n",
      "Parsing AF31.pdf\n",
      "Parsing AF02.pdf\n",
      "Parsing AY07.pdf\n",
      "Parsing AZ07.pdf\n",
      "Parsing AY14.pdf\n",
      "Parsing AT05.pdf\n",
      "Parsing AW26.pdf\n",
      "Parsing AY15.pdf\n",
      "Parsing AZ04.pdf\n",
      "Parsing AT07.pdf\n",
      "Parsing AW12.pdf\n",
      "Parsing AW09.pdf\n",
      "Parsing AW06.pdf\n",
      "Parsing AT25.pdf\n",
      "Parsing AT24.pdf\n",
      "Parsing AW04.pdf\n",
      "Parsing AW34.pdf\n",
      "Parsing AT32.pdf\n",
      "Parsing AT19.pdf\n",
      "Parsing AT23.pdf\n",
      "Parsing AF29.pdf\n",
      "Parsing AF20.pdf\n",
      "Parsing AY11.pdf\n",
      "Parsing AW33.pdf\n",
      "Parsing AZ02.pdf\n",
      "Parsing AZ05.pdf\n",
      "Parsing AT29.pdf\n",
      "Parsing AT03.pdf\n",
      "Parsing AF01.pdf\n",
      "Parsing AT27.pdf\n",
      "Parsing AT34.pdf\n",
      "Parsing AY08.pdf\n",
      "Parsing AF11.pdf\n",
      "Parsing AT14.pdf\n",
      "Parsing AF26.pdf\n",
      "Parsing AW22.pdf\n",
      "Parsing AW15.pdf\n",
      "Parsing AT15.pdf\n",
      "Parsing AT35.pdf\n",
      "Parsing AF17.pdf\n",
      "Parsing AF19.pdf\n",
      "Parsing AY09.pdf\n",
      "Parsing AW02.pdf\n",
      "Parsing AF04.pdf\n",
      "Parsing AW25.pdf\n",
      "Parsing AW21.pdf\n",
      "Parsing AF07.pdf\n",
      "Parsing AT10.pdf\n",
      "Parsing AW16.pdf\n",
      "Parsing AT01.pdf\n",
      "Parsing AF10.pdf\n",
      "Parsing AT33.pdf\n",
      "Parsing AT18.pdf\n",
      "Parsing AT20.pdf\n",
      "Parsing AW30.pdf\n",
      "Parsing AF08.pdf\n",
      "Parsing AW19.pdf\n",
      "Parsing AY01.pdf\n",
      "Parsing AZ09.pdf\n",
      "Parsing AT26.pdf\n",
      "Parsing AZ03.pdf\n",
      "Parsing AF05.pdf\n",
      "Parsing AF39.pdf\n",
      "Parsing AF13.pdf\n",
      "Parsing AT28.pdf\n",
      "Parsing AW17.pdf\n",
      "Parsing AF22.pdf\n",
      "Parsing AW29.pdf\n",
      "Parsing AF25.pdf\n",
      "Parsing AW31.pdf\n",
      "Parsing AT04.pdf\n",
      "Parsing AW20.pdf\n",
      "Parsing AT16.pdf\n",
      "Parsing AZ10.pdf\n",
      "Parsing AY06.pdf\n",
      "Parsing AF14.pdf\n",
      "Parsing AF28.pdf\n",
      "Parsing AT11.pdf\n",
      "Parsing AT06.pdf\n",
      "Parsing AF35.pdf\n",
      "Parsing AT08.pdf\n",
      "Parsing AW27.pdf\n",
      "Parsing AY13.pdf\n",
      "Parsing AW14.pdf\n",
      "Parsing AY10.pdf\n",
      "Parsing AT12.pdf\n",
      "Parsing AF23.pdf\n",
      "Parsing AT31.pdf\n",
      "Parsing AY04.pdf\n",
      "Parsing AT30.pdf\n",
      "Parsing AW01.pdf\n",
      "Parsing AT22.pdf\n",
      "Parsing AZ06.pdf\n",
      "Parsing AY03.pdf\n",
      "Parsing AW11.pdf\n",
      "Parsing AF41.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "def parse_pdfs_from_folder(folder_path):\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            print(f\"Parsing {filename}\")\n",
    "            with pdfplumber.open(os.path.join(folder_path, filename)) as pdf:\n",
    "                text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    text += page.extract_text()\n",
    "                all_texts.append(text)\n",
    "    return all_texts\n",
    "\n",
    "folder_path = \"./data/A_document\"\n",
    "texts = parse_pdfs_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看部分 PDF 文档内容，并检查是否解析正确 (如文本顺序是否正确、表格内容是否正确等)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 检查第 1 页标题:\n",
      " 中国联合网络通信股份有限公司 2019年半年度报告摘要\n",
      "公司代码：600050 公司简称：中国联通\n",
      "\n",
      "\n",
      ">>> 检查第 2 页 2.2 表格:\n",
      " 2中国联合网络通信股份有限公司 2019年半年度报告摘要\n",
      "2.2 公司主要财务数据\n",
      "单位：元 币种：人民币\n",
      "本报告期末\n",
      "本报告期末 上年度末\n",
      "比上年度末增减(%)\n",
      "总资产 566,546,628,977 541,762,283,984 4.6\n",
      "归属于上市公司股东的净资产 141,411,066,2\n"
     ]
    }
   ],
   "source": [
    "# texts[3] 对应 AY12.pdf\n",
    "print(\">>> 检查第 1 页标题:\\n\", texts[3][0:50])\n",
    "print(\"\\n>>> 检查第 2 页 2.2 表格:\\n\", texts[3][801:950])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 检查第 1 页标题:\n",
      " No.202327\n",
      "全球数字经济白皮书\n",
      "(2023 年)\n",
      "中国信息通信研究院\n",
      "2024年1月版权声明\n",
      "\n",
      ">>> 检查第 17 页 (里面有一张图片，没有被解析):\n",
      " 字经济持续渗透。受行业属\n",
      "性等因素影响，从全球看，数字技术在传统产业的应用率先在第三\n",
      "产业爆发、数字化效果最显著，在第二产业的应用效果有待持续释\n",
      "放，在第一产业的应用受到自然条件、土地资源等因素限制，仍需\n",
      "探索更加适合的数字化解决方案。2022 年，全球 51 个主要经济体第\n",
      "三、二、一产业数字经济增加值占行业增加值比重分别为 45.7%、\n",
      "24.7%和 9.1%，分别较去年提升 0.7、0.5 和 0.2 个百分点。\n",
      "来源：中国信息通信研究院\n",
      "图22022年全球数字经济整体发展情况\n",
      "（二）全球数字经济多极化趋势进一步深化\n",
      "整体看，中、美、欧基于市场、技术、规则等方面优势，持续\n",
      "加大数字经济发\n"
     ]
    }
   ],
   "source": [
    "# texts[116] 对应 AZ06.pdf，里面包含一些图片内容\n",
    "print(\">>> 检查第 1 页标题:\\n\", texts[116][0:50])\n",
    "print(\"\\n>>> 检查第 17 页 (里面有一张图片，没有被解析):\\n\", texts[116][14400:14700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 检查第 1 页标题:\n",
      " 本文档为2024CCFBDCI比赛用语料的一部分。部分文档使用大语言模型改写生成，内容可能与现实情况\n",
      "不符，可能不具备现实意义，仅允许在本次比赛中使用。\n",
      "学思践悟担使命 走在前列建新功 ——中国联通党员干部热议深入开展主题教育感悟收获\n",
      "发布时间：2023-07-31发布人：新闻宣\n",
      "\n",
      ">>> 检查第 2 页最后的内容:\n",
      " 动发展战略总体要求、\n",
      "重要遵循和工作要求，努力探索实现金融与通信跨界创新成果落地，将数字人民币金融基础设施与信息技术基础设施相结合，\n",
      "推动联通SIM卡硬钱包产品广泛应用于消费、交通、校园等场景，助力主业高质量发展。\n",
      "万里征程风正劲，千钧重任再扬帆。中国联通将持续高质量开展主题教育，坚持学思用贯通、知信行统一，把习近平新时\n",
      "代中国特色社会主义思想落实到改造主观世界和客观世界的实践中，切实把学习成果转化为坚定理想信念、落实战略规划和推\n",
      "进中国联通高质量发展的强大力量。\n"
     ]
    }
   ],
   "source": [
    "# texts[101] 对应 AT11.pdf\n",
    "print(\">>> 检查第 1 页标题:\\n\", texts[101][0:140])\n",
    "print(\"\\n>>> 检查第 2 页最后的内容:\\n\", texts[101][3300:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🚀 改进建议：**\n",
    "\n",
    "在观察解析后的文档内容后，可以总结一些缺陷并进行优化，例如:\n",
    "\n",
    "- 去掉 '\\n'，使句子更加连续通顺 （如果 '\\n' 前面是 '。'，可以保留）\n",
    "\n",
    "- 去掉 '本文档为2024CCFBDCI比赛用语料的一部分。部分文档使用大语言模型改写生成，内容可能与现实情况\\n不符，可能不具备现实意义，仅允许在本次比赛中使用。\\n'\n",
    "\n",
    "- 用 ``text += page.extract_text()`` 解析出的表格不够明确，可以尝试 ``table += page.extract_tables()`` 得到更加格式化的表格\n",
    "    \n",
    "    - 这样的表格可以独立于 `text`，单独保存到 `all_tables` 中\n",
    "    \n",
    "    - 此外，答案存在于文档表格中的问题对应的**难度系数分**更高，能更好的利用表格数据是一个提分点\n",
    "\n",
    "- ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 文本分块 (Chunk)\n",
    "\n",
    "这里给出一种简单的实现方式，使用 ``LangChain`` 中推荐的 `RecursiveCharacterTextSplitter`，按顺序分隔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 4105\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 分块函数\n",
    "def chunk_texts(texts, chunk_size=500, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        chunks.extend(splitter.split_text(text))\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_texts(texts, chunk_size=300, chunk_overlap=100)\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['本文档为2024CCFBDCI比赛用语料的一部分。部分文档使用大语言模型改写生成，内容可能与现实情况\\n不符，可能不具备现实意义，仅允许在本次比赛中使用。\\n中国联通发布经济社会智能孪生系统服务经济治理效能提升\\n发布时间： 发布人：新闻宣传中心\\n7 月 20 日，2024 中国联20通24合-0作7伙-2伴2大会人工智能赋能数据要素论坛在上海\\n世博中心举办，论坛上中国联通智慧足迹总经理李振军与多位院士专家共同发布\\n经济社会智能孪生系统。\\n高层次专家云集，共同见证重要发布\\n本次发布会邀请了多位学术界和行业界的顶尖专家参与。其中包括加拿大工',\n",
       " '世博中心举办，论坛上中国联通智慧足迹总经理李振军与多位院士专家共同发布\\n经济社会智能孪生系统。\\n高层次专家云集，共同见证重要发布\\n本次发布会邀请了多位学术界和行业界的顶尖专家参与。其中包括加拿大工\\n程院院士、加拿大工程研究院院士、欧洲科学院院士、郑州大学学术副校长杨天\\n若，中国金融四十人论坛学术顾问许宪春，中国社会科学院国家未来城市实验室\\n主任刘治，厦门大学经济学院、王亚南经济研究院院长周莹刚，国家发展和改革\\n委员会经济与技术经济研究所室副主任杨威等重量级嘉宾。他们的到来不仅为本\\n次发布会增添了学术和专业的光彩，更为经济社会智能孪生系统的应用和发展提\\n供了坚实的理论支持和智力保障。',\n",
       " '委员会经济与技术经济研究所室副主任杨威等重量级嘉宾。他们的到来不仅为本\\n次发布会增添了学术和专业的光彩，更为经济社会智能孪生系统的应用和发展提\\n供了坚实的理论支持和智力保障。\\n紧跟政策导向，服务国家战略需求\\n在当前复杂多变的全球经济环境下，中国经济正面临着前所未有的挑战和机\\n遇。二十届三中全会明确提出要在经济发展和社会民生等重点领域进一步深化改\\n革，以应对全球化进程中的各种不确定因素。中国联通作为国家战略的重要参与\\n者，始终将服务国家战略需求作为自身发展的核心使命。基于多年在数据和通信\\n领域的积累与实践，中国联通智慧足迹推出的经济社会智能孪生系统，旨在通过']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🚀 改进建议：**\n",
    "\n",
    "- 对 `chunk_size` 和 `chunk_overlap` 进行调整\n",
    "\n",
    "- 使用其他文本分块方法，可以参考：\n",
    "\n",
    "    - [LangChain v0.3 Documentation - Text-Splitters](https://python.langchain.com/docs/how_to/#text-splitters)\n",
    "\n",
    "    - [[A tutorial on Github] 5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "- ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 用 向量模型 对每个 Chunk 进行向量化处理，并存储到向量数据库\n",
    "\n",
    "首先下载**比赛规定**的向量模型（Embedding model）: [bge-large-zh-v1.5](https://www.modelscope.cn/models/AI-ModelScope/bge-large-zh-v1.5)\n",
    "\n",
    "- 下载时间 ~ 4 min\n",
    "\n",
    "- 存储空间 ~ 2.5G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 17:40:51,399 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ea0fc405354607acfe92de376fe28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [1_Pooling/config.json]:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06751eea5a534382a35a10014634981b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/0.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79edb5303034e738c6f3215ffc91e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config_sentence_transformers.json]:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6c2ab145a04c9ea526929ac4d75a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/47.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f546bb9f052d48e6a0d0bf117dff40e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198be19345ad4961b56feb0fe9b7f75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [modules.json]:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b0da3dc4d04d129d0e433fc978b886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [pytorch_model.bin]:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fb04b71de240f5ad5c67b56e75536c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/27.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b825cfcd7c04f929958aa3eba1734fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [sentence_bert_config.json]:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35afd0bf77544440a31d33fd1efb6ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [special_tokens_map.json]:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6850969ff014e9eb0b14f0820a914cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/429k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e500f98c06c546b089a3bf3ced1023d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e768d7865948cc9087ddf9cda24645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.txt]:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/lujd/modelscope/hub/AI-ModelScope/bge-large-zh-v1___5\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "to_your_path = '/data/lujd/modelscope/hub' \n",
    "embed_model_dir = snapshot_download('AI-ModelScope/bge-large-zh-v1.5', revision='master',\n",
    "                                cache_dir=to_your_path)      # cache_dir 默认是 '~/.cache/modelscope/hub'\n",
    "print(embed_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后使用 ``langchain`` 中的 ``HuggingFaceBgeEmbeddings`` 加载 `bge-large-zh-v1.5` 模型\n",
    "\n",
    "- 还有其他加载模型的方式，参考 [bge-large-zh-v1.5 - Usage](https://www.modelscope.cn/models/AI-ModelScope/bge-large-zh-v1.5#Usage)\n",
    "\n",
    "- 如果加载到 GPU 上，占显存空间 ~ 1800M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='/data/lujd/modelscope/hub/AI-ModelScope/bge-large-zh-v1___5' cache_folder=None model_kwargs={'device': 'cuda:4'} encode_kwargs={'normalize_embeddings': True} query_instruction='为这个句子生成表示以用于检索相关文章：' embed_instruction='' show_progress=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embed_model_name = embed_model_dir\n",
    "embed_model_kwargs = {'device': 'cuda:4'}           # 'cuda'/'cuda:4' for gpu\n",
    "encode_kwargs = {'normalize_embeddings': True}      # set True to compute cosine similarity\n",
    "embed_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=embed_model_name,\n",
    "    model_kwargs=embed_model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"为这个句子生成表示以用于检索相关文章：\"\n",
    ")\n",
    "print(embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试**向量模型**\n",
    "\n",
    "- 输入 `submit_example.csv` 中提供的 `answer`，检查输出向量与文件中提供的 `embedding` 是否一致\n",
    "\n",
    "    - 也就是看是否正确使用了主办方规定的向量模型\n",
    "\n",
    "- 跑 5 个 chunk (chunk_size=300): ~ 2000M（GPU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1024 [-0.027079833671450615, -0.009818877093493938, -0.01249589491635561, -0.003817810909822583, 0.017168041318655014, 0.027743525803089142, 0.03685056418180466, -0.0333469994366169, -0.040052466094493866, 0.028761744499206543]\n"
     ]
    }
   ],
   "source": [
    "submit_example_answer1 = [\"我们坚定践行网络强国、数字中国、智慧社会战略部署，今天的中国联通，正在从传统运营商加速向数字科技领军企业转变，实现了四个维度的转型升级：一是联接规模和联接结构升维，从过去的连接人为主拓展到连接人机物，大力发展物联网和工业互联网；二是核心功能升维，从以基础连接为主发展到大联接、大计算、大数据、大应用、大安全五大主责主业；三是服务和赋能水平升维，以5G、云计算、大数据、人工智能、区块链为代表的新一代信息技术和实体经济的结合，服务数字政府、数字社会、数字经济的能力不断增强；四是发展理念升维，我们以传统的市场驱动为主转变为市场驱动和创新驱动相结合的发展模式，尤其是加大了科技创新及人才方面的投入力度，创新发展的动能得到了空前的释放。\"]\n",
    "embeddings = embed_model.embed_documents(submit_example_answer1)\n",
    "print(len(embeddings), len(embeddings[0]), embeddings[0][:10])\n",
    "\n",
    "# 与 submit_example.csv 中第 1 个 问题的 answer embedding 是一致的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1024)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embed_model.embed_documents(chunks[:5])\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对所有 chunks 进行向量化处理，并存储到向量数据库 （``FAISS``）\n",
    "\n",
    "- 占显存空间 ~ 2600M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stored vectors: 4105\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 为每个 chunk 添加一个 doc_id，方便查找。\n",
    "documents = [Document(page_content=chunk, metadata={'doc_id': i}) for i, chunk in enumerate(chunks)]\n",
    "\"\"\"\n",
    "还可以添加更多信息用于后面的检索策略，例如属于哪个文档、文档标题等。\n",
    "\"\"\"\n",
    "\n",
    "vector_store = FAISS.from_documents(documents, embed_model)\n",
    "print(f\"Number of stored vectors: {vector_store.index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 根据问题检索答案（文本召回）\n",
    "\n",
    "### 4.1. 检索与问题关联度最高的文本\n",
    "\n",
    "计算问题的向量，然后在向量数据库中进行相似度计算并排序，返回前 K 个（Top-K）最相似的文本块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, vector_store, model, top_k=5):\n",
    "    query_embedding = model.embed_query(query)\n",
    "    retrieved_docs = vector_store.similarity_search_by_vector(query_embedding, top_k)\n",
    "    return retrieved_docs\n",
    "\n",
    "import pandas as pd\n",
    "submit_df = pd.read_csv('./data/A_question.csv', sep=',')\n",
    "\n",
    "quesid2docs_dict = {}\n",
    "for ind, row in submit_df.iterrows():\n",
    "    quesid2docs_dict[row['ques_id']] = retrieve_docs(row['question'], vector_store, embed_model, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察最后一个问题以及检索到的文本，会发现一些可以改进的点：\n",
    "\n",
    "- “本文档为2024CCFBDCI比赛用语料的一部分。部分文档使用大语言模型改写生成，....” 这段与文档内容无关的话出现在 Top-1 文本中\n",
    "\n",
    "- 排名第 3 的文本给出了正确答案：“.....分别是中国银行、国航、伊利集团、安踏和中国联通。” 而 Top-1 没有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'截至2017年12月26日，北京市2022年冬奥会和冬残奥会官方合作伙伴已达五家，分别是哪几家？'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['question']             # 最后一个问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': 2519}, page_content='本文档为2024CCFBDCI比赛用语料的一部分。部分文档使用大语言模型改写生成，内容可能与现实情况\\n不符，可能不具备现实意义，仅允许在本次比赛中使用。\\n中国联通成为北京2022年冬奥会和冬残奥会官方通信服务合作伙伴\\n发布时间：2017-12-26发布人：新闻发布人\\n12月26日晚间，在中国联通大厦里，中国联通的红色“中国结”与北京2022年冬奥会会徽和冬残奥会会徽的组合标志格外\\n醒目，中国联通与北京2022年冬奥会和冬残奥会组织委员会签约仪式在这里举行。这场签约，标志着中国联通正式成为北京\\n2022年冬奥会和冬残奥会官方通信服务合作伙伴，跻身于北京冬奥组委市场开发计划最高级别的赞助企业之列。'),\n",
       " Document(metadata={'doc_id': 2520}, page_content='2022年冬奥会和冬残奥会官方通信服务合作伙伴，跻身于北京冬奥组委市场开发计划最高级别的赞助企业之列。\\n当晚，北京市副市长、北京冬奥组委执行副主席张建东，中国残联理事长、北京冬奥组委副主席鲁勇；中国联通董事长王\\n晓初，中国联通总经理陆益民，中国联通副总经理邵广禄出席签约仪式。北京冬奥组委秘书长韩子荣与中国联通副总经理买彦\\n州代表双方签署赞助协议。\\n北京市副市长、北京冬奥组委执行副主席张建东致辞表示，北京冬奥会是我国重要历史节点的重大标志性活动，对于展现\\n新时代国家形象、促进国家发展、振奋民族精神，具有重大而深远的意义。与通信服务企业开展合作，不仅是实施市场开发计'),\n",
       " Document(metadata={'doc_id': 2523}, page_content='致的速率支撑奥运，以智慧的应用、丰富的产品服务奥运，以智慧的技术、专业的队伍保障奥运，为冬奥会的成功举办、为中\\n国力量的再次彰显，注入强劲新动能。\\n在签约仪式上，中国联通正式宣布启用166新号段，在全国投入百万个号码，以优质的网络、产品与服务持续助力中国通\\n信事业和中国奥运事业。\\n据了解，北京2022年冬奥会和冬残奥会市场开发计划自今年2月份启动以来，得到了社会各界的广泛关注和广大企业的踊\\n跃参与。目前，北京2022年冬奥会和冬残奥会官方合作伙伴已达五家，分别是中国银行、国航、伊利集团、安踏和中国联通。'),\n",
       " Document(metadata={'doc_id': 3489}, page_content='匠心网络服务 为冬奥保驾护航\\n据杜永红表示，自成为北京2022年冬奥会和冬残奥会唯一官方通信服务合作伙伴后，中国\\n联通的冬奥网络始终按照北京、张家口“两地三赛区一张网”的核心理念进行规划，确保冬奥区\\n域通信服务标准一致，为全面实施京津冀协同发展战略起到引领作用。\\n三年多来，中国联通以“智慧冬奥”为总目标，以匠心品质搭建奥运保障总体框架，完成\\n首份冬奥通信遗产《冬奥通信服务指南》，并圆满完成了世界转播商大会、世界媒体大会、冬奥\\n倒计时500天、国际冬季单项体育联合会赛道认证保障等一系列冬奥重保任务，获得了北京冬奥\\n组委的高度称赞。'),\n",
       " Document(metadata={'doc_id': 2521}, page_content='新时代国家形象、促进国家发展、振奋民族精神，具有重大而深远的意义。与通信服务企业开展合作，不仅是实施市场开发计\\n划的重要内容，也是高质量、高效率推进筹办工作的迫切需要。中国联通是我国电信事业的领军企业，拥有庞大的通信服务资\\n源和丰富的重大活动服务保障经验。北京冬奥组委将携手中国联通加强北京冬奥会与国内外各界的相联相通，努力打造智慧冬\\n奥、科技冬奥；同时，也会努力搭建推广平台，提供良好服务，使中国联通充分获得奥运品牌带来的广泛收益，助力企业更好\\n发展。\\n中国联通董事长王晓初在致辞中表示，近年来，在“网络强国”“数字中国”“互联网+行动计划”等国家战略的引领下，中国联')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quesid2docs_dict[100]       # 最后一个问题检索到的文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🚀 改进建议：**\n",
    "\n",
    "- 使用其他的相似性指标\n",
    "\n",
    "- 使用其他的检索策略，例如先过滤再检索、EnsembleRetriever 等，参考：[LangChain v0.3 Documentation - Retrievers](https://python.langchain.com/docs/how_to/#retrievers)\n",
    "\n",
    "- 检索到 Top-K 文本后，再使用其他指标/模型/方法对这 Top-K 文本进行排序 （重排）\n",
    "\n",
    "- ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 如何将检索的文本变为问题的答案？\n",
    "\n",
    "这里直接用贪心策略将检索到的 Top-1 文本作为答案，并给出相应的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_and_embedding(retrieved_docs):\n",
    "    # for example, select top1 retrieved_docs as the final answer\n",
    "    top1_doc = retrieved_docs[0]\n",
    "\n",
    "    # 文本内容\n",
    "    answer = top1_doc.page_content\n",
    "\n",
    "    # 根据前面设置的 doc_id 获取文本对应的向量\n",
    "    doc_id = top1_doc.metadata['doc_id']\n",
    "    embedding = vector_store.index.reconstruct(doc_id)\n",
    "\n",
    "    return answer, embedding\n",
    "\n",
    "quesid2answer_dict, quesid2embedding_dict = {}, {}\n",
    "for quesid, retrieved_docs in quesid2docs_dict.items():\n",
    "    answer, embedding = get_answer_and_embedding(retrieved_docs)\n",
    "    quesid2answer_dict[quesid] = answer                 # str\n",
    "    quesid2embedding_dict[quesid] = embedding           # np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🚀 改进建议：**\n",
    "\n",
    "- 将检索到的 Top-K 文本拼起来作为答案（但要保证不能太长，防止被惩罚），有如下考虑：\n",
    "\n",
    "    - Top-1 未必是完整答案，可能是 Top-1 和 Top-2 拼起来。\n",
    "\n",
    "    - 一些难度系数为 2 的问题，答案涉及“文档多个段落”或者“多个文档”，需要将多个文本块合并后返回。\n",
    "\n",
    "- ......\n",
    "\n",
    "- 如果想用 LLM 且条件允许，可以：\n",
    "    \n",
    "    - 将检索到的 Top-K 文本和 Query 写入 LLM 的 prompt template 中，然后输入到 LLM 中，推理得到一个更加综合自然的回答。但是比赛要求禁止使用 LLM 直接生成最终答案。\n",
    "\n",
    "    - 所以用 LLM 的回答作为 reference，计算其与 Top-K 文本的相似度，然后将 Top-K 文本重新排序（重排），最后将重新排序后的 Top-1 作为答案。（**可能存在犯规风险**）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存结果\n",
    "\n",
    "将召回的答案及向量写入 `submit_df`，并保存到 `./submission/` 下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ques_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>根据年度报告，2022年中国联通在向数字科技领军企业转变的过程中实现了哪些维度的转型升级？</td>\n",
       "      <td>数字经济的不断深入发展，中国联通将继续秉持创新驱动的发展理念，不断提升数据管理能力和智能化水...</td>\n",
       "      <td>-0.011659057, -0.016988184, -0.040713284, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>告诉我2022年联通产业互联网收入的同比增长速度。</td>\n",
       "      <td>家庭产品，促进相互拉动发展。受激烈竞争影响，2019 年上半年，公司固网宽带接入收入同比下降...</td>\n",
       "      <td>-0.009771062, -0.041889433, -0.022890702, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ques_id                                       question  \\\n",
       "0        1  根据年度报告，2022年中国联通在向数字科技领军企业转变的过程中实现了哪些维度的转型升级？   \n",
       "1        2                      告诉我2022年联通产业互联网收入的同比增长速度。   \n",
       "\n",
       "                                              answer  \\\n",
       "0  数字经济的不断深入发展，中国联通将继续秉持创新驱动的发展理念，不断提升数据管理能力和智能化水...   \n",
       "1  家庭产品，促进相互拉动发展。受激烈竞争影响，2019 年上半年，公司固网宽带接入收入同比下降...   \n",
       "\n",
       "                                           embedding  \n",
       "0  -0.011659057, -0.016988184, -0.040713284, -0.0...  \n",
       "1  -0.009771062, -0.041889433, -0.022890702, -0.0...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REMOVE_NEWLINE = True\n",
    "\n",
    "if REMOVE_NEWLINE:      # 避免对方测试时使用其他读取方式，而导致读取错误\n",
    "    submit_df['answer'] = submit_df['ques_id'].apply(lambda x: quesid2answer_dict[x].replace(\"\\n\", \"\"))\n",
    "else:\n",
    "    submit_df['answer'] = submit_df['ques_id'].apply(lambda x: quesid2answer_dict[x])\n",
    "\n",
    "# embedding 注意与 submit_example.csv 的格式一致\n",
    "submit_df['embedding'] = submit_df['ques_id'].apply(lambda x: str(list(quesid2embedding_dict[x]))[1:-1])\n",
    "\n",
    "submit_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved path: ./submission/submit_1001_2128.csv\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# 可以按日期-时间保存，例如 \"2024-10-01 21:28\" 记为 \"1001_2128\"\n",
    "today = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "submit_savepath = f'./submission/submit_{today}.csv'\n",
    "submit_df.to_csv(submit_savepath, sep=',', index=False)\n",
    "print(f\"Saved path: {submit_savepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
